{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0fd28c",
   "metadata": {},
   "source": [
    "# Prediction of Nighttime NO2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b01399",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e260b76d",
   "metadata": {},
   "source": [
    "### Names and Acronyms\n",
    "1) ASDC: [Atmosperhic Science Data Center](https://asdc.larc.nasa.gov/about)\n",
    "1) PGN (Pandora): [Pandonia Global Network](https://www.pandonia-global-network.org/) / [Pandora](https://pandora.gsfc.nasa.gov/About/)\n",
    "    - **NOTE**: NASA's portion of the PGN is known as Pandora.  Within the scope of this notebook, Pandora and PGN may be used interchangably as this project will only use NASAs PGN site data.\n",
    "1) TEMPO: [Troposoperic Emissions: Monitoring of Pollution](https://science.nasa.gov/mission/tempo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4999c",
   "metadata": {},
   "source": [
    "### Resources\n",
    "1) ASDC Data Processing Tool (Version 1)\n",
    "    - This notebook was published by the ASDC and provides examples of how to correctly load and use Pandora and TEMPO data.\n",
    "    - https://github.com/nasa/ASDC_Data_and_User_Services/blob/main/TEMPO/additional_drafts/ASDC_Data_Processing_ML_v1.2.ipynb\n",
    "1) PGN Station Map\n",
    "    - A map showing the location of all PGN groundsites.\n",
    "    - https://blickm.hetzner.pandonia-global-network.org/livemaps/pgn_stationsmap.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c7064",
   "metadata": {},
   "source": [
    "This notebook borrows heavily from and extens the functionality of the NASA, ASDC Data and User Servicies notebook found here:\n",
    "\n",
    "https://github.com/nasa/ASDC_Data_and_User_Services/blob/main/TEMPO/additional_drafts/ASDC_Data_Processing_ML_v1.2.ipynb\n",
    "\n",
    "This notebook intends to test the hypothesis that a model can be built with Pandora which can predict nightitme NO<sub>2</sub> and that that model can be applied to TEMPO daytime measurments to predict NO<sub>2</sub> for any location covered by TEMPO.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7cdc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1657c",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "There are many tools available such as [poetry](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://python-poetry.org/&ved=2ahUKEwjr9aLgna6QAxX5EVkFHVsNBMUQFnoECBsQAQ&usg=AOvVaw3Jp8q7OO7XkcY8Tq4tDe30) and [uv](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://docs.astral.sh/uv/&ved=2ahUKEwiP9aXVna6QAxVyF1kFHeyTNGYQFnoECAsQAQ&usg=AOvVaw2VJVt0jrah2S9tIgdc1yRc) that simplify and speed up environment setup.  For simplicity, this guide only covers the method built into the python standard library.\n",
    "1) Install [Python 3.11](https://www.python.org/downloads/) (or higher)\n",
    "1) (Recomended) Create a virtual environment (learn more [here](https://docs.python.org/3/library/venv.html))\n",
    "1) Install the required packages using the following command.<br>`% pip install pyproject.toml`\n",
    "1) Select the newly created kernal in your notebook.\n",
    "    - NOTE: this varies slightly between notebook tools, but in almost all tools you will be prompted to select a kernal upon running a cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c5de3",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "79c24beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import earthaccess\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cbfbc",
   "metadata": {},
   "source": [
    "### Data Access\n",
    "In order to access data, you will need an Earthdata Login account.  If you do not have an Earthdata Login account, you can create one here:<br>\n",
    "https://urs.earthdata.nasa.gov/\n",
    "\n",
    "The earthaccess module allows you to authenticate.  Multilple login options exist for providing your credentials, you can read more on options here:<br>\n",
    "https://pypi.org/project/earthaccess/<br>\n",
    "By unless another option is configured, you will be prompted by your notebook to enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a082e562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x14e4e776410>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa5d17",
   "metadata": {},
   "source": [
    "Notebook Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae862668",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_DATA_PATH = Path('pgn-data')\n",
    "PGN_DATA_PATH.mkdir(mode=0o777, parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d02eaa",
   "metadata": {},
   "source": [
    "## 1. Data Prepairation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114510e5",
   "metadata": {},
   "source": [
    "### 1.1. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e3f556",
   "metadata": {},
   "source": [
    "#### 1.1.1. Define Download Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e13d742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_range = []\n",
    "spatial_range = []\n",
    "\n",
    "sites = ['BronxNY', 'BuffaloNY', 'QueensNY']\n",
    "sites_url = \"https://data.pandonia-global-network.org\"\n",
    "file_suffix = \"rnvh3p1-8.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640f721",
   "metadata": {},
   "source": [
    "#### 1.1.1. Download Pandora data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648d329d",
   "metadata": {},
   "source": [
    "Get sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f56e73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(url: str):\n",
    "    \"\"\"\n",
    "    An tool for getting all PGN links from a PGN data webpage (https://data.hetzner.pandonia-global-network.org/)\n",
    "\n",
    "    ARGS:\n",
    "        url (str): The URL of the page to extract links form\n",
    "    \"\"\"\n",
    "    things_to_remove = [\n",
    "        '<span class=\"name\">',\n",
    "        '</span>',\n",
    "        '/</span>'\n",
    "    ]\n",
    "\n",
    "    response = requests.get(url)\n",
    "    assert response.status_code==200, f\"Download failed with code {response.status_code}\"\n",
    "    \n",
    "    # get item name lines\n",
    "    names = [l.strip() for l in response.text.splitlines()]\n",
    "    names = [l for l in names if l.startswith('<span class=\"name\">')]\n",
    "\n",
    "    # get item names from name lines\n",
    "    for thing_to_remove in things_to_remove:\n",
    "        names = [l.replace(thing_to_remove, '') for l in names]\n",
    "        names = [l.rstrip('/') for l in names]\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca2aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting File URLs\n",
      "Site 1 of 3: BronxNY\n",
      "\tInstrument 1 of 2: Pandora147s1\n",
      "\tInstrument 2 of 2: Pandora180s1\n",
      "Site 2 of 3: BuffaloNY\n",
      "\tInstrument 1 of 1: Pandora206s1\n",
      "Site 3 of 3: QueensNY\n",
      "\tInstrument 1 of 1: Pandora55s1\n",
      "File URLs:\n",
      "\thttps://data.pandonia-global-network.org/BronxNY/Pandora147s1/L2/Pandora147s1_BronxNY_L2_rnvh3p1-8.txt\n",
      "\thttps://data.pandonia-global-network.org/BronxNY/Pandora180s1/L2/Pandora180s1_BronxNY_L2_rnvh3p1-8.txt\n",
      "\thttps://data.pandonia-global-network.org/BuffaloNY/Pandora206s1/L2/Pandora206s1_BuffaloNY_L2_rnvh3p1-8.txt\n",
      "\thttps://data.pandonia-global-network.org/QueensNY/Pandora55s1/L2/Pandora55s1_QueensNY_L2_rnvh3p1-8.txt\n"
     ]
    }
   ],
   "source": [
    "# get file URLs\n",
    "print(\"Getting File URLs\")\n",
    "pgn_urls: list[str] = []\n",
    "for i, site in enumerate(sites):\n",
    "    site_url = f\"{sites_url}/{site}\"\n",
    "    instruments = get_page_links(site_url)\n",
    "    print(f\"Site {i+1} of {len(sites)}:\", site)\n",
    "    for j, instrument in enumerate(instruments):\n",
    "        print(f\"\\tInstrument {j+1} of {len(instruments)}:\", instrument)\n",
    "        instrument_url = f\"{site_url}/{instrument}/L2/{instrument}_{site}_L2_{file_suffix}\"\n",
    "        pgn_urls.append(instrument_url)\n",
    "\n",
    "print(\"File URLs:\")\n",
    "for pgn_url in pgn_urls:\n",
    "    print(f\"\\t{pgn_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dc28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pgn files to pgn-data\n",
      "Downloading file 1 of 4: Pandora147s1_BronxNY_L2_rnvh3p1-8.txt\n",
      "Downloading file 2 of 4: Pandora180s1_BronxNY_L2_rnvh3p1-8.txt\n",
      "Downloading file 3 of 4: Pandora206s1_BuffaloNY_L2_rnvh3p1-8.txt\n",
      "Downloading file 4 of 4: Pandora55s1_QueensNY_L2_rnvh3p1-8.txt\n",
      "Files downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download Files\n",
    "pgn_paths: list[Path] = []\n",
    "print(f\"Downloading pgn files to {PGN_DATA_PATH}\")\n",
    "for i, pgn_url in enumerate(pgn_urls):\n",
    "    file_name = Path(pgn_url).name\n",
    "    print(f\"Downloading file {i+1} of {len(pgn_urls)}: {file_name}\")\n",
    "    response = requests.get(pgn_url)\n",
    "    assert response.ok, f\"File not found: {pgn_url}\"\n",
    "\n",
    "    file_path = PGN_DATA_PATH.joinpath(file_name)\n",
    "    file_path.write_bytes(response.content)\n",
    "    pgn_paths.append(file_path)\n",
    "print(\"Files downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5d840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pgn-data\\Pandora147s1_BronxNY_L2_rnvh3p1-8.txt has 93 lines in the header.\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 81 fields in line 98, saw 109\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[143]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(pgn_path, \u001b[33m\"\u001b[39m\u001b[33mhas\u001b[39m\u001b[33m\"\u001b[39m, header_lines, \u001b[33m\"\u001b[39m\u001b[33mlines in the header.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpgn_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheader_lines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\Documents\\Classes\\MST\\MLA\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\Documents\\Classes\\MST\\MLA\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\Documents\\Classes\\MST\\MLA\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\natha\\Documents\\Classes\\MST\\MLA\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 81 fields in line 98, saw 109\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following PGN data extraction functions were copied from ASDC_Data_Processing_ML_v1.2.ipynb and will not be changed\n",
    "\"\"\"\n",
    "##unction reading Pandora NO2 data files rnvh3p1-8\n",
    "# function converting Pandora timestamp into a set of  year, month, day, hour, minute, and second\n",
    "# function read_timestamp converts Pandora timestamp of the format\n",
    "# 'yyyymmddThhmmssZ' into a set of 6 numbers:\n",
    "# integer year, month, day, hour, minute, and real second.\n",
    "def read_timestamp(timestamp):\n",
    "\n",
    "  yyyy = int(timestamp[0:4])\n",
    "  mm = int(timestamp[4:6])\n",
    "  dd = int(timestamp[6:8])\n",
    "  hh = int(timestamp[9:11])\n",
    "  mn = int(timestamp[11:13])\n",
    "  ss = int(timestamp[13:15])\n",
    "\n",
    "  return yyyy, mm, dd, hh, mn, ss\n",
    "\n",
    "\n",
    "# function reading Pandora NO2 data file rnvh3p1-8\n",
    "#\n",
    "# Below is the second version of function read_Pandora_NO2_rnvs3p1_8. It is to be used for the future validation efforts.\n",
    "# The difference with the original version is that instead of discriminating negative values of the total NO2 column,\n",
    "# it uses quality flags. It was previously found that QF == 0 does not occure often enough,\n",
    "# so we will have to use QF == 10 (not-assured high quality).\n",
    "#\n",
    "# function read_Pandora_NO2_rnvh3p1_8 reads Pandora total NO2 column data files ending with rnvh3p1-8.\n",
    "# Arguments:\n",
    "# fname - name file to be read, string;\n",
    "# start_date - beginning of the time interval of interest,\n",
    "#              integer of the form YYYYMMDD;\n",
    "# end_date -   end of the time interval of interest,\n",
    "#              integer of the form YYYYMMDD.\n",
    "#\n",
    "# if start_date is greater than end_date, the function returns a numpy array\n",
    "# with shape (0, 8), otherwise it returns an 8-column numpy array\n",
    "# with with columns being year, month, day, hour, minute, second of observation\n",
    "# and retrieved total NO2 column along with its total uncertainty.\n",
    "#\n",
    "# NO2 column is in mol/m^2, so conversion to molecules/cm^2 is performed by\n",
    "# multiplication by Avogadro constant, NA =  6.02214076E+23, and division by 1.E+4\n",
    "def read_Pandora_NO2_rnvh3p1_8(fname, start_date, end_date):\n",
    "\n",
    "  conversion_coeff = 6.02214076E+19 # Avogadro constant divided by 10000\n",
    "\n",
    "  data = np.empty([0, 8])\n",
    "  if start_date > end_date: return -999., -999., data\n",
    "\n",
    "  with codecs.open(fname, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "    while True:\n",
    "# Get next line from file\n",
    "      line = f.readline()\n",
    "\n",
    "      if line.find('Short location name:') >= 0:\n",
    "        loc_name = line.split()[-1] # location name, to be used in the output file name\n",
    "        print('location name ', loc_name)\n",
    "\n",
    "      if line.find('Location latitude [deg]:') >= 0:\n",
    "        lat = float(line.split()[-1]) # location latitude\n",
    "        print('location latitude ', lat)\n",
    "\n",
    "      if line.find('Location longitude [deg]:') >= 0:\n",
    "        lon = float(line.split()[-1]) # location longitude\n",
    "        print('location longitude ', lon)\n",
    "\n",
    "      if line.find('--------') >= 0: break\n",
    "\n",
    "    while True:\n",
    "# Get next line from file\n",
    "      line = f.readline()\n",
    "      # print(line)\n",
    "      if line.find('--------') >= 0: break\n",
    "\n",
    "    while True:\n",
    "# now reading line with data\n",
    "      line = f.readline()\n",
    "      \n",
    "      if not line: break\n",
    "\n",
    "      line_split = line.split()\n",
    "     \n",
    "      yyyy, mm, dd, hh, mn, ss = read_timestamp(line_split[0])\n",
    "      date_stamp = yyyy*10000 + mm*100 + dd\n",
    "      if date_stamp < start_date or date_stamp > end_date: continue\n",
    "\n",
    "      QF = int(line_split[52]) # total column uncertainty\n",
    "\n",
    "      if QF == 0 or QF == 10:\n",
    "        column = float(line_split[61]) # Nitrogen dioxide tropospheric vertical column amount [moles per square meter]\n",
    "        column_unc = float(line_split[62]) # Independent uncertainty of nitrogen dioxide tropospheric vertical column amount [moles per square meter]\n",
    "        \n",
    "        data = np.append(data, [[yyyy, mm, dd, hh, mn, ss\\\n",
    "                               , column*conversion_coeff\\\n",
    "                               , column_unc*conversion_coeff]], axis = 0)\n",
    "\n",
    "  return lat, lon, loc_name, data\n",
    "\n",
    "def read_Pandora_NO2_rnvm2p1_8(fname, start_date, end_date):  #####################LUNAR\n",
    "  conversion_coeff = 6.02214076E+19 # Avogadro constant divided by 10000\n",
    "  data = np.empty([0, 8])\n",
    "  if start_date > end_date: return -999., -999., data\n",
    "\n",
    "  # with codecs.open(fname, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "  with codecs.open(fname, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "\n",
    "    while True:\n",
    "# Get next line from file\n",
    "      line = f.readline()\n",
    "\n",
    "      if line.find('Short location name:') >= 0:\n",
    "        loc_name = line.split()[-1] # location name, to be used in the output file name\n",
    "        # print('location name ', loc_name)\n",
    "\n",
    "      if line.find('Location latitude [deg]:') >= 0:\n",
    "        lat = float(line.split()[-1]) # location latitude\n",
    "        # print('location latitude ', lat)\n",
    "\n",
    "      if line.find('Location longitude [deg]:') >= 0:\n",
    "        lon = float(line.split()[-1]) # location longitude\n",
    "        # print('location longitude ', lon)\n",
    "\n",
    "      if line.find('--------') >= 0: break\n",
    "\n",
    "    while True:\n",
    "# Get next line from file\n",
    "      line = f.readline()\n",
    "      # print(line)\n",
    "      if line.find('--------') >= 0: break\n",
    "\n",
    "    while True:\n",
    "# now reading line with data\n",
    "      line = f.readline()\n",
    "      \n",
    "      if not line: break\n",
    "\n",
    "      line_split = line.split()\n",
    "     \n",
    "      yyyy, mm, dd, hh, mn, ss = read_timestamp(line_split[0])\n",
    "      date_stamp = yyyy*10000 + mm*100 + dd\n",
    "      if date_stamp < start_date or date_stamp > end_date: continue\n",
    "\n",
    "      QF = int(line_split[35])\n",
    "      \n",
    "      if QF == 0 or QF == 10 or QF == 1 or QF ==11:\n",
    "      # if QF:\n",
    "        column = float(line_split[38])# - float(line_split[52]) # Nitrogen dioxide total vertical column amount [moles per square meter]\n",
    "        column_unc = float(line_split[39]) # Independent uncertainty of nitrogen dioxide tropospheric vertical column amount [moles per square meter]\n",
    "        \n",
    "        data = np.append(data, [[yyyy, mm, dd, hh, mn, ss\\\n",
    "                               , column*conversion_coeff\\\n",
    "                               , column_unc*conversion_coeff]], axis = 0)\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef878a3",
   "metadata": {},
   "source": [
    "#### 1.1.1 Download TEMPO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b95b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32408bb7",
   "metadata": {},
   "source": [
    "### 1.2. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ea48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bde00571",
   "metadata": {},
   "source": [
    "### 1.3. Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f1694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d08698",
   "metadata": {},
   "source": [
    "### 1.4. Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba6d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08645d08",
   "metadata": {},
   "source": [
    "## 2. Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819fdd0",
   "metadata": {},
   "source": [
    "### 2.1 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d32961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2ff45a",
   "metadata": {},
   "source": [
    "### 2.2 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77b91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b7fe43",
   "metadata": {},
   "source": [
    "### 2.3. Model Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bd37ff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
